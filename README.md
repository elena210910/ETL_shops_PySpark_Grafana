# ETL_shops_PySpark_Grafana

## Descripci√≥n de la tarea
Trabajo en una empresa que gestiona una gran tienda en l√≠nea.
Mi tarea es crear una canalizaci√≥n automatizada (Pipeline) para el procesamiento 
y an√°lisis de datos de ventas.

## Stack tecnol√≥gico
- **PostgreSQL**: para almacenamiento de datos.
- **ClickHouse**: para operaciones anal√≠ticas.
- **Apache Airflow**: para orquestaci√≥n de tareas.
- **PySpark**: para procesamiento de datos.
- **Grafana**: para visualizaci√≥n de datos finales.

Dado que los datos reales son parte confidencial de la empresa, 
generar√© datos realistas de ventas. Luego, cargar√© esos datos en una tabla de PostgreSQL, 
y despu√©s los procesar√©, limpiar√©, cargar√© en bases de datos y realizar√© operaciones anal√≠ticas.

## *Pasos de realizacion*.üöÄ

### Instalaci√≥n de Docker y Docker Compose

1. **Instalaci√≥n de Docker**: - Descarga e instala  [Dockerfile](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/Docker/Dockerfile).
2. **Instalaci√≥n de Docker Compose**: - Descarga e instala [Docker Compose](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/Docker/docker-compose).



### DAG

EL script que define un [DAG](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/Dag) en Apache Airflow con siete tareas diferentes: 
generar datos, procesar datos, limpiar datos, cargar en PostgreSQL, cargar en ClickHouse,
realizar operaciones anal√≠ticas. Las tareas est√°n 
configuradas para ejecutarse en un orden espec√≠fico usando operadores de Python.

## Conexi√≥n a la base de datos

"IMPORTANTE: [Con√©ctate a la base de datos](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/admin.PNG) a trav√©s de ADMIN en la interfaz de AIRFLOW.
Las contrase√±as y otros detalles est√°n especificados en el archivo docker-compose

## Ejecutamos nuestro DAG en Airflow


**Podemos ver c√≥mo nuestras tareas se completaron con √©xito!**



![](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/dag_succes.PNG)



### Y ahora, veamos los resultados..

1. Ejemplo de datos generados - [sales_data.csv](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/ej_csv.PNG)
   
2. Los datos cargados en [PostgreSQL](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/postgres.PNG) (en un total son dos tablas).
   
3. CLickHouse - Atreves de una consulta simple de SQL (SELECT *
                                                       FROM aggregated_sales_data asd 
                                                       ORDER BY product_id 
                                                       LIMIT 10)
   podemos observar q datos han sido [cargados](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/click_tab.PNG).

4. Visualicemos los datos obtenidos en [GRAFANA](https://github.com/elena210910/ETL_shops_PySpark_Grafana/blob/main/chart_grafana.PNG).
   Conect√©monos a la tabla en ClickHouse - Veamos en qu√© regi√≥n est√°n los compradores m√°s activos üöÄ


