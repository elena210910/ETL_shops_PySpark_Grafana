import requests
import psycopg2
import subprocess
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.utils.dates import days_ago
import random
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType, FloatType, DateType, StringType
from pyspark.sql import SparkSession
import pyspark.sql.functions as F 


# Parámetros predeterminados para DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'retries': 1,
    'catchup': False,
}

# Definición del DAG
dag = DAG(
    'sales_data_pipeline',
    default_args=default_args,
    description='Пайплайн обработки данных о продажах',
    schedule_interval="45 9 * * 2",  # 12:45 по Москве (UTC+3) каждый вторник
)

# Función para verificar la conexión con PostgreSQL
def query_postgres(**kwargs):
    command = [
        'psql',
        '-h', 'postgres_user',
        '-U', 'user',
        '-d', 'test',
        '-c', 'SELECT version();'
    ]
    env = {"PGPASSWORD": "password"}

    try:
        result = subprocess.run(command, env=env, capture_output=True, text=True, check=True)
        print(f"PostgreSQL version: {result.stdout}")
    except subprocess.CalledProcessError as e:
        print(f"Failed to connect to PostgreSQL, error: {e.stderr}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# Función para generar ventas aleatorias
def generate_sales_data():
    regions = ["North", "South", "East", "West"]
    num_sales = 100
    start_date = datetime.now() - timedelta(days=365)
    sales_data = []

    for i in range(num_sales):
        sale_id = i + 1
        customer_id = random.randint(1, 1000)
        product_id = random.randint(1, 20)# что бы были повторяющиеся купленные товары
        quantity = random.randint(1, 7)
        sale_date = (start_date + timedelta(days=random.randint(0, 365))).date()
        sale_amount = round(quantity * random.uniform(3, 38), 2)
        region = random.choice(regions)
        
        sales_data.append((sale_id, customer_id, product_id, quantity, sale_date, sale_amount, region))

    with open('/opt/airflow/df/sales_data.csv', 'w') as f:
        f.write('sale_id,customer_id,product_id,quantity,sale_date,sale_amount,region\n')
        for sale in sales_data:
            f.write(','.join(map(str, sale)) + '\n')

# Función para la limpieza y el procesamiento de datos
def process_sales_data():
    spark = (SparkSession.builder
             .appName("SalesAnalysis")
             .config("spark.jars", "/opt/airflow/jars/postgresql-42.2.23.jar") # Укажи путь к драйверу PostgreSQL
             .config("spark.driver.extraClassPath", "/opt/airflow/jars/postgresql-42.2.23.jar")
             .config("spark.executor.extraClassPath", "/opt/airflow/jars/postgresql-42.2.23.jar")
             .getOrCreate())
             
    df = spark.read.csv('/opt/airflow/df/sales_data.csv', header=True, inferSchema=True)
    
    df = df.dropDuplicates()
    df = df.withColumn("sale_id", df["sale_id"].cast(IntegerType()))
    df = df.withColumn("customer_id", df["customer_id"].cast(IntegerType()))
    df = df.withColumn("product_id", df["product_id"].cast(IntegerType()))
    df = df.withColumn("quantity", df["quantity"].cast(IntegerType()))
    df = df.withColumn("sale_amount", df["sale_amount"].cast(FloatType()))
    df = df.withColumn("sale_date", df["sale_date"].cast(DateType()))
    df = df.withColumn("region", df["region"].cast(StringType()))

    df.write \
      .format("jdbc") \
      .option("url", "jdbc:postgresql://postgres_user:5432/test") \
      .option("dbtable", "cleaned_sales_data") \
      .option("user", "user") \
      .option("password", "password") \
      .mode("overwrite") \
      .save()



#Función para la agregacion
def aggregate_sales_data():
    # Создание Spark-сессии
    spark = (SparkSession.builder
             .appName("SalesAnalysis")
             .config("spark.jars", "/opt/airflow/jars/postgresql-42.2.23.jar") # Укажи путь к драйверу PostgreSQL
             .config("spark.driver.extraClassPath", "/opt/airflow/jars/postgresql-42.2.23.jar")
             .config("spark.executor.extraClassPath", "/opt/airflow/jars/postgresql-42.2.23.jar")
             .getOrCreate())

    # Lectura de datos de la tabla cleaned_sales_data en PostgreSQL
    df = spark.read.format("jdbc") \
        .option("url", "jdbc:postgresql://postgres_user:5432/test") \
        .option("dbtable", "cleaned_sales_data") \
        .option("user", "user") \
        .option("password", "password") \
        .load()

     # Aggregación de datos
    aggregated_df = df.groupBy("region", "product_id") \
        .agg(
            F.count("sale_id").alias("total_sales"),
            F.round(F.sum("sale_amount"), 2).alias("total_amount"),
            F.round(F.avg("sale_amount"), 2).alias("average_sale_amount")
        )

    # Escritura de datos agregados en PostgreSQL
    aggregated_df.write \
        .format("jdbc") \
        .option("url", "jdbc:postgresql://postgres_user:5432/test") \
        .option("dbtable", "aggregated_sales_data") \
        .option("user", "user") \
        .option("password", "password") \
        .mode("overwrite") \
        .save()

    

# Creacion de la tabla en clickhouse
def create_clickhouse_table():
    query = """
    CREATE TABLE IF NOT EXISTS aggregated_sales_data (
       region String,
       product_id Int32,
       total_sales Int32,
       total_amount Float32,
       average_sale_amount Float32,
       import_date Date
       ) ENGINE = MergeTree()
       ORDER BY (region, product_id);"""
    response = requests.post('http://clickhouse_user:8123', params={'query': query})

    if response.status_code==200:
        print("Table created successfully")
    else:
        print(f"Error creating table: {response.text}")  

def load_to_clickhouse():
    # Conneccion a PostgreSQL
    conn = psycopg2.connect(
        dbname="test",
        user="user",
        password="password",
        host="postgres_user",
        port="5432"
    )
    cur = conn.cursor() 
    cur.execute("SELECT region, product_id, total_sales, total_amount, average_sale_amount FROM aggregated_sales_data")

    # Obtenemos la fecha de importación
    import_date = datetime.now().strftime('%Y-%m-%d')

    # Formación de datos para cargar en ClickHouse
    data = []
    for row in cur.fetchall():  
        region, product_id, total_sales, total_amount, average_sale_amount = row
        data.append(f"('{region}', {product_id}, {total_sales}, {total_amount}, {average_sale_amount}, '{import_date}')")

    data_str = ",".join(data)
    print(f"Data to be loaded into ClickHouse: {data_str}") # Логирование

    # Limpieza de la tabla antes de insertar datos nuevos
    truncate_query = "TRUNCATE TABLE aggregated_sales_data"
    requests.post("http://clickhouse_user:8123", params={'query': truncate_query})
    
    # Consulta para insertar en ClickHouse
    query = f"INSERT INTO aggregated_sales_data (region, product_id, total_sales, total_amount, average_sale_amount, import_date) VALUES {data_str}"
    response = requests.post("http://clickhouse_user:8123", params={'query': query})
    if response.status_code == 200:
        print("Data loaded successfully")
    else:
        print(f"Error loading data: {response.text}")

    cur.close()
    conn.close()

# Tarea 1 para verificar la conexión con PostgreSQL
query_postgres_task = PythonOperator(
    task_id='query_postgres',
    python_callable=query_postgres,
    dag=dag,
)

# Tarea 2 para crear una tabla en PostgreSQL
create_table = PostgresOperator(
    task_id='create_table',
    postgres_conn_id='postgres_default',
    sql="""
    CREATE TABLE IF NOT EXISTS cleaned_sales_data (
        sale_id INT,
        customer_id INT,
        product_id INT,
        quantity INT,
        sale_date DATE,
        sale_amount FLOAT,
        region VARCHAR(50)
    );
    """,
    dag=dag,
)


# Tarea 3 Generacion de los datos
generate_sales_data_task = PythonOperator(
    task_id='generate_sales_data',
    python_callable=generate_sales_data,
    dag=dag,
)

# Tarea 4 procesamiento de los datos
process_sales_data_task = PythonOperator(
    task_id='process_sales_data',
    python_callable=process_sales_data,
    dag=dag,
)

# Tarea 5 de agregacion de los datos
aggregate_sales_data_task = PythonOperator(
     task_id='aggregate_sales_data', 
     python_callable=aggregate_sales_data, 
     dag=dag, )


# Tarea 6 Creacion de la tabla en ClickHouse
create_clickhouse_table_task = PythonOperator(
    task_id='create_clickhouse_table',
    python_callable=create_clickhouse_table,
    dag=dag,
)
# Tarea 7 Añadimos datos a la tabla
load_to_clickhouse_task = PythonOperator(
    task_id='load_to_clickhouse',
    python_callable=load_to_clickhouse,
    dag=dag,
)

# Определение последовательности задач
query_postgres_task >> create_table >> generate_sales_data_task >> process_sales_data_task >> aggregate_sales_data_task>> create_clickhouse_table_task >> load_to_clickhouse_task

